{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7488072,"sourceType":"datasetVersion","datasetId":4359562},{"sourceId":7488077,"sourceType":"datasetVersion","datasetId":4359567}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to my notebook.  This is my first attempt at a Kaggle competition.  In order to get a better sense of the data, I took a decidedly basic approach for this first attempt at this competition, where I used the training data just for my own edification, studying familiar patterns.  This employs little to no contextural analysis, and instead simply evaluates each word/set of words and attempts to spot names or familiar patterns.  I decided to ignore usernames and addresses as there simply was not enough training data to help me pick up on specific patterns there.  \n\nFrom here, I plan to examine some of the great work others have put together and hopefully, combined with my now intiminate knowledge of the training data, I will be able to add some real value on the predictions side of things.\n\nI nevertheless feel this can be helpful for certain users, as it demonstrates a simple approach to this problem in a very digestable manner.  This performance (0.66) also establishes a useful baseline, as this is achieved without using the training data (notice the training data is never referenced throughout the code), leveraging instead only patterns I spot within the data.\n\nFeedback welcome!","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# load the test data\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\n# transformed_data will convert the full data set into one broken down by token\ntransformed_data = []\n\n# each entry in the data has an associated doc_id and set of tokens\n# here we create an array where each row consists of a separate token with its associated doc_id, and\n# we add an associated row_id that refers to the position of each token within a given entry\nfor entry in data:\n    tokens = entry['tokens']\n    doc_id = entry['document']\n    i = 0\n    for token in tokens:\n        transformed_data.append({'token': token, 'document': doc_id, 'row_id': i})\n        i += 1\n\n# convert transformed_data into a dataframe\ndf = pd.DataFrame(transformed_data)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-27T20:00:47.443772Z","iopub.execute_input":"2024-01-27T20:00:47.444219Z","iopub.status.idle":"2024-01-27T20:00:47.474231Z","shell.execute_reply.started":"2024-01-27T20:00:47.444189Z","shell.execute_reply":"2024-01-27T20:00:47.473193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\nimport re\n\n# words.words() is a list of words from nltk.corpus.  We use this to distinguish between names and words. \n# we use set() so that searching for a unique word is O(1)\nword_set = set(words.words())\n\n# in this simple approach, we use regex for identifying email, url_personal, id_num, and phone_num\n# for phone num, we classify begin (B) and inner (I) later; for the others, we assume always (B), as to\n# an extent demonstrated in the training data.   \ndef find_string_type(string):\n    patterns = {\n        'B-EMAIL': r'[\\w\\.-]+@[\\w\\.-]+',\n        'B-URL_PERSONAL': r'http[s]?://[^\\s]+',\n        'B-ID_NUM': r'\\b\\d{7,}[A-Za-z]{0,3}\\b|\\b[A-Za-z]{0,3}\\d{7,}\\b',\n        'PHONE_NUM': r'\\b(?:\\+?\\d{1,3}[\\s.-]?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}(?:\\s*(?:ext|x)\\s*\\d{2,6})?\\b',\n    }\n\n    for key, pattern in patterns.items():\n        if re.search(pattern, string):\n            return key\n\n    return 'O'\n\n# in the training data, certain titles get flagged as names, so we use this limited list to help strip this out\nTITLES = [\n    'dr',   \n    'mr',    \n    'mrs',   \n    'ms',    \n    'mx',    \n    'sir',  \n    'madam', \n    'miss',  \n    'prof',  \n    'professor',\n    'rev',   \n    'hon',   \n    'lt',    \n    'capt', \n    'cdr',   \n    'col',   \n    'gen',   \n    'judge', \n    'justice' \n]\n\n\n# determines whether a given word is common, i.e. not a name\ndef is_common_word(word):\n    return word.lower() in word_set or word.lower() in TITLES\n\n# checks whether a given token is a name.  if so, add the corresponding label.\n# in reviewing the training data, first names always come before last names,\n# which is handled in this method\ndef check_names(first_name, last_name, document, cur_document, token, cur_token):\n    if first_name.istitle() and isinstance(last_name, str) and last_name.istitle():\n        search_first_name = first_name in first_names\n        search_last_name = last_name in last_names\n        if search_first_name and search_last_name:\n            label.append('B-NAME_STUDENT')\n            label.append('I-NAME_STUDENT')\n            append_others(document, cur_document, token, cur_token)\n            append_others(document, next_document, token, next_token)\n            return True, True\n\n    elif first_name.istitle():\n        search_first_name = first_name in first_names\n        if search_first_name:\n            label.append('B-NAME_STUDENT')\n            append_others(document, cur_document, token, cur_token)\n            return False, True\n\n    return False, False\n\n# for appending things other than the label (document, token) to the submission file\ndef append_others(document, cur_document, token, cur_token):\n    document.append(cur_document)\n    token.append(cur_token)\n    \n# to hold the information that goes into the submission file\ndocument, token, label = [], [], []\n\n# load the list of first and last names from JSON files\n# these files can be found here: https://github.com/philipperemy/name-dataset\nfirst_names = json.load(open(\"/kaggle/input/first-names-json/first_names.json\"))\nlast_names = json.load(open(\"/kaggle/input/first-names-json/first_names.json\"))\n\n# flags for helping drive the code logic\nfound_start = False\ncontinue_next = False\nfound_name = False\nfound_phone = False\n\n# for each row in our dataframe\nfor idx, row in df.iterrows():\n    if idx == len(df) - 1:\n        break\n    \n    # we retrieve this info to be submitted to the submission csv when applicable\n    first_name = row['token']\n    last_name = df.at[idx+1, 'token']\n    cur_document = row['document']\n    next_document = df.at[idx+1, 'document']\n    cur_token = row['row_id']\n    next_token = df.at[idx+1, 'row_id']\n\n    # if on the last row we found this row's label, skip to the next row\n    if continue_next:\n        continue_next = False\n        continue\n\n    # only proceed if this token is a string\n    if isinstance(first_name, str):\n        if not is_common_word(first_name):\n            continue_next, found_name = check_names(first_name, last_name, document, cur_document, token, cur_token)\n\n            if found_name:\n                continue\n\n            new_label = find_string_type(first_name)\n            next_label = find_string_type(last_name)\n            \n            # in examining the training data, I found certain patterns for phone numbers, which I have accounted for here\n            if not found_phone:\n                if first_name == '(' and next_label == 'PHONE_NUM':\n                    label.append('B-PHONE_NUM')\n                    label.append('I-PHONE_NUM')\n                    append_others(document, cur_document, token, cur_token)\n                    append_others(document, next_document, token, next_token)\n                    \n                    found_phone = True\n\n                elif new_label != 'O':\n                    starter = ''\n                    if new_label == 'PHONE_NUM':\n                        starter = 'B-'\n                    label.append(starter + new_label)                        \n                    append_others(document, cur_document, token, cur_token)\n            else:\n                if first_name == '-':\n                    label.append('I-PHONE_NUM')                        \n                    append_others(document, cur_document, token, cur_token)\n                else:\n                    starter = ''\n                    if new_label == 'PHONE_NUM':\n                        starter = 'I-'\n                    label.append(starter + new_label)                        \n                    append_others(document, cur_document, token, cur_token)\n\n# create a dataframe with just the information needed for submission                    \ndf = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label\n})\n\n# add the row_id\ndf[\"row_id\"] = list(range(len(df)))                    \n\n# submit\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:03:31.615922Z","iopub.execute_input":"2024-01-27T20:03:31.617140Z","iopub.status.idle":"2024-01-27T20:03:49.869473Z","shell.execute_reply.started":"2024-01-27T20:03:31.617091Z","shell.execute_reply":"2024-01-27T20:03:49.868264Z"},"trusted":true},"execution_count":null,"outputs":[]}]}